# Fine_Tuning-

Flux LoRA Fine-Tuning : A training pipeline for fine-tuning Flux.1-dev with LoRA (Low-Rank Adaptation) using custom image datasets and prompts.
This setup is designed for efficient training on GPUs, supporting mixed precision, multiple resolutions, and integrated sampling.
ðŸš€ Features


LoRA Training â€“ Efficient parameter-efficient fine-tuning with rank 16.


Multi-Resolution Support â€“ Training with [512, 768, 1024] resolutions for better generalization.


Trigger Word Support â€“ Automatically injects a trigger word (luthara) into captions and prompts.


Quantization & bf16 â€“ Optimized for GPUs with mixed precision and reduced VRAM usage.


FlowMatch Noise Scheduler â€“ Specialized noise scheduler for Flux training.


Integrated Sampling â€“ Gener
ates preview samples at regular intervals using defined prompts.


ðŸ›  Tech Stack


Python â€“ Core programming language


Diffusers â€“ Flux training integration


LoRA â€“ Parameter-efficient fine-tuning


PyTorch â€“ Deep learning framework


AdamW8bit â€“ Optimized memory-efficient optimizer


FAISS-style bucketing â€“ Multi-resolution handling for training


Hugging Face Hub (optional) â€“ Push trained models to HF repo
